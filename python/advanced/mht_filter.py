#!/usr/bin/env python3
"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
NX-MIMOSA - MHT (MULTIPLE HYPOTHESIS TRACKING) IMPLEMENTATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Multiple Hypothesis Tracking for complex multi-target scenarios.
Deferred decision logic for optimal track association.

Features:
- Full hypothesis tree management
- N-scan pruning for computational tractability
- Hypothesis merging for similar tracks
- Track-oriented MHT (TOMHT) implementation
- Integration with UKF/CKF base filters
- Automatic track initiation and deletion

Mathematical Foundation:
- Reid (1979). "An Algorithm for Tracking Multiple Targets"
- Blackman & Popoli. "Design and Analysis of Modern Tracking Systems"
- Bar-Shalom et al. "Tracking and Data Fusion", 2011

Author: Dr. Mladen MeÅ¡ter / Nexellum d.o.o.
License: AGPL v3 / Commercial
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import numpy as np
from scipy.stats import chi2
from typing import List, Tuple, Dict, Optional, Any, Set
from dataclasses import dataclass, field
from enum import IntEnum
from collections import defaultdict
import heapq
import copy

__version__ = "1.0.0"
__author__ = "Dr. Mladen MeÅ¡ter"


# =============================================================================
# CONFIGURATION
# =============================================================================

@dataclass
class MHTConfig:
    """MHT configuration parameters."""
    # Gating
    gate_probability: float = 0.9999
    gate_threshold: float = None
    
    # Detection model
    pd: float = 0.9                      # Detection probability
    lambda_fa: float = 1e-6              # False alarm density
    lambda_new: float = 1e-7             # New target density
    
    # Hypothesis management
    n_scan_pruning: int = 3              # N-scan depth for pruning
    max_hypotheses: int = 100            # Maximum global hypotheses
    max_tracks_per_hypothesis: int = 50  # Maximum tracks per hypothesis
    hypothesis_prune_threshold: float = 0.01  # Prune hypotheses below this
    
    # Track management
    m_of_n_init: Tuple[int, int] = (2, 3)  # M-of-N for track initiation
    n_miss_delete: int = 5                  # Consecutive misses for deletion
    min_track_probability: float = 0.1      # Minimum track existence probability
    
    # Merging
    merge_distance_threshold: float = 100.0  # meters
    merge_velocity_threshold: float = 10.0   # m/s
    
    def __post_init__(self):
        if self.gate_threshold is None:
            self.gate_threshold = chi2.ppf(self.gate_probability, df=3)


# =============================================================================
# DATA STRUCTURES
# =============================================================================

class TrackStatus(IntEnum):
    """Track status."""
    TENTATIVE = 0
    CONFIRMED = 1
    DELETED = 2


@dataclass
class TrackState:
    """Track state within a hypothesis."""
    track_id: int
    state: np.ndarray
    covariance: np.ndarray
    status: TrackStatus = TrackStatus.TENTATIVE
    
    # Track history
    n_updates: int = 0
    n_misses: int = 0
    consecutive_misses: int = 0
    
    # Predicted values
    predicted_state: np.ndarray = None
    predicted_covariance: np.ndarray = None
    innovation_covariance: np.ndarray = None
    
    # Score
    log_likelihood: float = 0.0
    existence_probability: float = 0.5
    
    def copy(self) -> 'TrackState':
        """Create a deep copy."""
        return TrackState(
            track_id=self.track_id,
            state=self.state.copy(),
            covariance=self.covariance.copy(),
            status=self.status,
            n_updates=self.n_updates,
            n_misses=self.n_misses,
            consecutive_misses=self.consecutive_misses,
            predicted_state=self.predicted_state.copy() if self.predicted_state is not None else None,
            predicted_covariance=self.predicted_covariance.copy() if self.predicted_covariance is not None else None,
            innovation_covariance=self.innovation_covariance.copy() if self.innovation_covariance is not None else None,
            log_likelihood=self.log_likelihood,
            existence_probability=self.existence_probability
        )


@dataclass
class Hypothesis:
    """Global hypothesis containing a set of tracks."""
    hypothesis_id: int
    tracks: Dict[int, TrackState]           # track_id -> TrackState
    log_probability: float = 0.0
    scan_history: List[Dict[int, int]] = field(default_factory=list)  # List of {track_id: meas_id}
    
    @property
    def probability(self) -> float:
        """Get normalized probability (for display)."""
        return np.exp(self.log_probability)
    
    def copy(self) -> 'Hypothesis':
        """Create a deep copy."""
        return Hypothesis(
            hypothesis_id=self.hypothesis_id,
            tracks={tid: t.copy() for tid, t in self.tracks.items()},
            log_probability=self.log_probability,
            scan_history=copy.deepcopy(self.scan_history)
        )


@dataclass
class Measurement:
    """Measurement for MHT."""
    measurement_id: int
    z: np.ndarray
    R: np.ndarray
    timestamp: float = 0.0


# =============================================================================
# TRACK ORIENTED MHT
# =============================================================================

class MHTFilter:
    """
    Track-Oriented Multiple Hypothesis Tracker (TOMHT).
    
    Maintains multiple global hypotheses, each representing a possible
    association history. Uses N-scan pruning to manage complexity.
    
    Usage:
        mht = MHTFilter(config)
        
        # Process measurements each scan
        measurements = [Measurement(i, z, R) for i, z in enumerate(sensor_data)]
        confirmed_tracks = mht.process(measurements, dt=1.0)
        
        # Get best hypothesis tracks
        best_tracks = mht.get_best_tracks()
    """
    
    def __init__(self, config: MHTConfig = None):
        """
        Initialize MHT filter.
        
        Args:
            config: MHT configuration
        """
        self.config = config or MHTConfig()
        
        # Hypothesis management
        self.hypotheses: List[Hypothesis] = []
        self._next_hypothesis_id = 0
        self._next_track_id = 0
        
        # Scan counter
        self.scan_count = 0
        
        # Initialize with empty hypothesis
        self.hypotheses.append(Hypothesis(
            hypothesis_id=self._get_next_hypothesis_id(),
            tracks={},
            log_probability=0.0
        ))
    
    def _get_next_hypothesis_id(self) -> int:
        """Get next hypothesis ID."""
        hid = self._next_hypothesis_id
        self._next_hypothesis_id += 1
        return hid
    
    def _get_next_track_id(self) -> int:
        """Get next track ID."""
        tid = self._next_track_id
        self._next_track_id += 1
        return tid
    
    def process(self, measurements: List[Measurement], dt: float) -> List[TrackState]:
        """
        Process a scan of measurements.
        
        Args:
            measurements: List of measurements
            dt: Time since last scan
            
        Returns:
            List of confirmed tracks from best hypothesis
        """
        self.scan_count += 1
        
        # Step 1: Predict all tracks in all hypotheses
        self._predict_all(dt)
        
        # Step 2: Gate measurements to tracks
        gating_matrix = self._compute_gating(measurements)
        
        # Step 3: Generate new hypotheses
        new_hypotheses = self._generate_hypotheses(measurements, gating_matrix)
        
        # Step 4: Prune hypotheses
        self.hypotheses = self._prune_hypotheses(new_hypotheses)
        
        # Step 5: N-scan pruning
        if self.scan_count >= self.config.n_scan_pruning:
            self._n_scan_pruning()
        
        # Step 6: Merge similar tracks
        self._merge_tracks()
        
        # Step 7: Track management (confirmation, deletion)
        self._manage_tracks()
        
        # Return confirmed tracks from best hypothesis
        return self.get_best_tracks()
    
    def _predict_all(self, dt: float):
        """Predict all tracks in all hypotheses."""
        for hyp in self.hypotheses:
            for track in hyp.tracks.values():
                # Simple CV prediction
                n_state = len(track.state)
                n_pos = n_state // 2
                
                F = np.eye(n_state)
                F[:n_pos, n_pos:] = np.eye(n_pos) * dt
                
                Q = np.zeros((n_state, n_state))
                q = 1.0  # Process noise intensity
                Q[:n_pos, :n_pos] = np.eye(n_pos) * q * dt**3 / 3
                Q[:n_pos, n_pos:] = np.eye(n_pos) * q * dt**2 / 2
                Q[n_pos:, :n_pos] = np.eye(n_pos) * q * dt**2 / 2
                Q[n_pos:, n_pos:] = np.eye(n_pos) * q * dt
                
                track.predicted_state = F @ track.state
                track.predicted_covariance = F @ track.covariance @ F.T + Q
                
                # Innovation covariance (for gating)
                n_meas = n_pos
                H = np.zeros((n_meas, n_state))
                H[:n_meas, :n_meas] = np.eye(n_meas)
                R = np.eye(n_meas) * 100  # Default, will be updated
                
                track.innovation_covariance = H @ track.predicted_covariance @ H.T + R
    
    def _compute_gating(self, measurements: List[Measurement]) -> Dict[Tuple[int, int, int], bool]:
        """
        Compute gating matrix.
        
        Returns dict of (hyp_id, track_id, meas_id) -> gated
        """
        gating = {}
        
        for hyp in self.hypotheses:
            for track in hyp.tracks.values():
                n_meas = len(measurements[0].z) if measurements else 3
                n_state = len(track.predicted_state)
                
                H = np.zeros((n_meas, n_state))
                H[:n_meas, :n_meas] = np.eye(n_meas)
                
                z_pred = H @ track.predicted_state
                S = track.innovation_covariance
                
                try:
                    S_inv = np.linalg.inv(S)
                except:
                    S_inv = np.linalg.pinv(S)
                
                for meas in measurements:
                    y = meas.z - z_pred
                    d2 = y.T @ S_inv @ y
                    
                    gating[(hyp.hypothesis_id, track.track_id, meas.measurement_id)] = \
                        d2 <= self.config.gate_threshold
        
        return gating
    
    def _generate_hypotheses(self, measurements: List[Measurement],
                             gating: Dict[Tuple[int, int, int], bool]) -> List[Hypothesis]:
        """
        Generate new hypotheses from all possible associations.
        """
        new_hypotheses = []
        
        for parent_hyp in self.hypotheses:
            # Get gated measurements for each track
            track_meas = defaultdict(list)
            for track in parent_hyp.tracks.values():
                for meas in measurements:
                    key = (parent_hyp.hypothesis_id, track.track_id, meas.measurement_id)
                    if gating.get(key, False):
                        track_meas[track.track_id].append(meas.measurement_id)
            
            # Generate associations
            # For simplicity, use greedy enumeration with limits
            associations = self._enumerate_associations(
                parent_hyp, measurements, track_meas
            )
            
            for assoc, log_prob in associations:
                # Create new hypothesis
                new_hyp = parent_hyp.copy()
                new_hyp.hypothesis_id = self._get_next_hypothesis_id()
                new_hyp.log_probability += log_prob
                
                # Update scan history
                scan_assoc = {}
                
                # Process associations
                used_meas = set()
                
                for track_id, meas_id in assoc.items():
                    if track_id not in new_hyp.tracks:
                        continue
                    
                    track = new_hyp.tracks[track_id]
                    
                    if meas_id is None:
                        # Missed detection
                        track.consecutive_misses += 1
                        track.n_misses += 1
                        scan_assoc[track_id] = -1
                    else:
                        # Update with measurement
                        meas = next(m for m in measurements if m.measurement_id == meas_id)
                        self._update_track(track, meas)
                        track.consecutive_misses = 0
                        track.n_updates += 1
                        used_meas.add(meas_id)
                        scan_assoc[track_id] = meas_id
                
                # Initialize new tracks from unassociated measurements
                for meas in measurements:
                    if meas.measurement_id not in used_meas:
                        # Create tentative track
                        new_track_id = self._get_next_track_id()
                        new_track = self._init_track(new_track_id, meas)
                        new_hyp.tracks[new_track_id] = new_track
                        scan_assoc[new_track_id] = meas.measurement_id
                        
                        # Log likelihood for new target
                        new_hyp.log_probability += np.log(self.config.lambda_new)
                
                new_hyp.scan_history.append(scan_assoc)
                new_hypotheses.append(new_hyp)
        
        return new_hypotheses
    
    def _enumerate_associations(self, hyp: Hypothesis, measurements: List[Measurement],
                                track_meas: Dict[int, List[int]]) -> List[Tuple[Dict[int, int], float]]:
        """
        Enumerate feasible associations.
        
        Returns list of (association_dict, log_probability)
        """
        track_ids = list(hyp.tracks.keys())
        n_tracks = len(track_ids)
        
        if n_tracks == 0:
            return [({}, 0.0)]
        
        associations = []
        meas_dict = {m.measurement_id: m for m in measurements}
        
        # Generate all valid assignments (including missed detections)
        def generate(idx, current_assoc, used_meas, log_prob):
            if idx >= n_tracks:
                associations.append((dict(current_assoc), log_prob))
                return
            
            if len(associations) >= self.config.max_hypotheses:
                return
            
            track_id = track_ids[idx]
            track = hyp.tracks[track_id]
            
            # Option 1: Missed detection
            miss_prob = np.log(1 - self.config.pd)
            generate(idx + 1, {**current_assoc, track_id: None},
                    used_meas, log_prob + miss_prob)
            
            # Option 2: Assign to gated measurement
            for meas_id in track_meas.get(track_id, []):
                if meas_id in used_meas:
                    continue
                
                meas = meas_dict[meas_id]
                det_log_prob = self._compute_detection_log_likelihood(track, meas)
                
                generate(idx + 1, {**current_assoc, track_id: meas_id},
                        used_meas | {meas_id}, log_prob + det_log_prob)
        
        generate(0, {}, set(), 0.0)
        
        return associations
    
    def _compute_detection_log_likelihood(self, track: TrackState, meas: Measurement) -> float:
        """Compute log likelihood of detection."""
        n_meas = len(meas.z)
        n_state = len(track.predicted_state)
        
        H = np.zeros((n_meas, n_state))
        H[:n_meas, :n_meas] = np.eye(n_meas)
        
        z_pred = H @ track.predicted_state
        y = meas.z - z_pred
        
        S = H @ track.predicted_covariance @ H.T + meas.R
        
        try:
            S_inv = np.linalg.inv(S)
            S_det = np.linalg.det(S)
        except:
            S_inv = np.linalg.pinv(S)
            S_det = max(1e-10, np.linalg.det(S))
        
        # Gaussian log likelihood
        log_norm = -0.5 * (n_meas * np.log(2 * np.pi) + np.log(max(S_det, 1e-10)))
        log_exp = -0.5 * (y.T @ S_inv @ y)
        
        return np.log(self.config.pd) + log_norm + log_exp
    
    def _update_track(self, track: TrackState, meas: Measurement):
        """Update track with measurement (Kalman update)."""
        n_meas = len(meas.z)
        n_state = len(track.predicted_state)
        
        H = np.zeros((n_meas, n_state))
        H[:n_meas, :n_meas] = np.eye(n_meas)
        
        z_pred = H @ track.predicted_state
        y = meas.z - z_pred
        
        S = H @ track.predicted_covariance @ H.T + meas.R
        
        try:
            S_inv = np.linalg.inv(S)
        except:
            S_inv = np.linalg.pinv(S)
        
        K = track.predicted_covariance @ H.T @ S_inv
        
        track.state = track.predicted_state + K @ y
        track.covariance = (np.eye(n_state) - K @ H) @ track.predicted_covariance
        
        # Update log likelihood
        track.log_likelihood += self._compute_detection_log_likelihood(track, meas)
    
    def _init_track(self, track_id: int, meas: Measurement) -> TrackState:
        """Initialize a new track from measurement."""
        n_meas = len(meas.z)
        n_state = n_meas * 2  # Position + velocity
        
        state = np.zeros(n_state)
        state[:n_meas] = meas.z
        
        covariance = np.eye(n_state)
        covariance[:n_meas, :n_meas] = meas.R
        covariance[n_meas:, n_meas:] = np.eye(n_meas) * 100  # Large velocity uncertainty
        
        return TrackState(
            track_id=track_id,
            state=state,
            covariance=covariance,
            status=TrackStatus.TENTATIVE,
            n_updates=1,
            predicted_state=state.copy(),
            predicted_covariance=covariance.copy()
        )
    
    def _prune_hypotheses(self, hypotheses: List[Hypothesis]) -> List[Hypothesis]:
        """Prune low-probability hypotheses."""
        if not hypotheses:
            return hypotheses
        
        # Normalize log probabilities
        max_log_prob = max(h.log_probability for h in hypotheses)
        for h in hypotheses:
            h.log_probability -= max_log_prob
        
        # Sort by probability (descending)
        hypotheses.sort(key=lambda h: h.log_probability, reverse=True)
        
        # Prune
        pruned = []
        total_prob = 0.0
        
        for h in hypotheses:
            if len(pruned) >= self.config.max_hypotheses:
                break
            
            prob = np.exp(h.log_probability)
            if prob < self.config.hypothesis_prune_threshold and len(pruned) > 0:
                continue
            
            pruned.append(h)
            total_prob += prob
        
        # Renormalize
        if total_prob > 0:
            log_total = np.log(total_prob)
            for h in pruned:
                h.log_probability -= log_total
        
        return pruned
    
    def _n_scan_pruning(self):
        """
        N-scan pruning: resolve associations older than N scans.
        
        Commits to the most likely association at scan (current - N).
        """
        if self.scan_count < self.config.n_scan_pruning:
            return
        
        # Find unique association histories at depth N
        history_groups = defaultdict(list)
        
        for hyp in self.hypotheses:
            if len(hyp.scan_history) >= self.config.n_scan_pruning:
                # Key is the association at scan (current - N)
                old_scan_idx = len(hyp.scan_history) - self.config.n_scan_pruning
                key = tuple(sorted(hyp.scan_history[old_scan_idx].items()))
                history_groups[key].append(hyp)
        
        # Keep only the best hypothesis for each history
        pruned = []
        for key, group in history_groups.items():
            best = max(group, key=lambda h: h.log_probability)
            pruned.append(best)
        
        # Add hypotheses without enough history
        for hyp in self.hypotheses:
            if len(hyp.scan_history) < self.config.n_scan_pruning:
                pruned.append(hyp)
        
        self.hypotheses = pruned
    
    def _merge_tracks(self):
        """Merge similar tracks within each hypothesis."""
        for hyp in self.hypotheses:
            track_ids = list(hyp.tracks.keys())
            merged = set()
            
            for i, tid1 in enumerate(track_ids):
                if tid1 in merged:
                    continue
                
                for tid2 in track_ids[i+1:]:
                    if tid2 in merged:
                        continue
                    
                    t1 = hyp.tracks[tid1]
                    t2 = hyp.tracks[tid2]
                    
                    # Check if tracks are similar
                    pos_diff = np.linalg.norm(t1.state[:3] - t2.state[:3])
                    vel_diff = np.linalg.norm(t1.state[3:] - t2.state[3:])
                    
                    if (pos_diff < self.config.merge_distance_threshold and
                        vel_diff < self.config.merge_velocity_threshold):
                        # Merge t2 into t1
                        self._merge_track_states(t1, t2)
                        merged.add(tid2)
            
            # Remove merged tracks
            for tid in merged:
                del hyp.tracks[tid]
    
    def _merge_track_states(self, t1: TrackState, t2: TrackState):
        """Merge track t2 into t1 using covariance intersection."""
        P1_inv = np.linalg.inv(t1.covariance)
        P2_inv = np.linalg.inv(t2.covariance)
        
        omega = 0.5  # Weighting factor
        
        P_merged_inv = omega * P1_inv + (1 - omega) * P2_inv
        P_merged = np.linalg.inv(P_merged_inv)
        
        x_merged = P_merged @ (omega * P1_inv @ t1.state + (1 - omega) * P2_inv @ t2.state)
        
        t1.state = x_merged
        t1.covariance = P_merged
        t1.n_updates += t2.n_updates
    
    def _manage_tracks(self):
        """Confirm tentative tracks and delete old tracks."""
        m, n = self.config.m_of_n_init
        
        for hyp in self.hypotheses:
            tracks_to_delete = []
            
            for track in hyp.tracks.values():
                # Confirmation: M-of-N
                if track.status == TrackStatus.TENTATIVE:
                    if track.n_updates >= m:
                        track.status = TrackStatus.CONFIRMED
                
                # Deletion: consecutive misses
                if track.consecutive_misses >= self.config.n_miss_delete:
                    tracks_to_delete.append(track.track_id)
            
            for tid in tracks_to_delete:
                del hyp.tracks[tid]
    
    def get_best_tracks(self) -> List[TrackState]:
        """Get confirmed tracks from the best hypothesis."""
        if not self.hypotheses:
            return []
        
        best_hyp = max(self.hypotheses, key=lambda h: h.log_probability)
        
        return [t for t in best_hyp.tracks.values() 
                if t.status == TrackStatus.CONFIRMED]
    
    def get_all_hypotheses(self) -> List[Hypothesis]:
        """Get all current hypotheses."""
        return self.hypotheses
    
    def get_track_probabilities(self) -> Dict[int, float]:
        """
        Compute track existence probabilities across all hypotheses.
        """
        track_probs = defaultdict(float)
        
        total_prob = sum(np.exp(h.log_probability) for h in self.hypotheses)
        
        for hyp in self.hypotheses:
            hyp_prob = np.exp(hyp.log_probability) / total_prob
            
            for track in hyp.tracks.values():
                if track.status == TrackStatus.CONFIRMED:
                    track_probs[track.track_id] += hyp_prob
        
        return dict(track_probs)


# =============================================================================
# MHT WITH IMM
# =============================================================================

class MHTIMM(MHTFilter):
    """
    MHT combined with Interacting Multiple Model.
    
    Each track hypothesis maintains multiple motion model hypotheses.
    """
    
    def __init__(self, config: MHTConfig = None,
                 n_modes: int = 3,
                 transition_matrix: np.ndarray = None):
        """
        Initialize MHT-IMM.
        
        Args:
            config: MHT configuration
            n_modes: Number of motion modes
            transition_matrix: Mode transition probability matrix
        """
        super().__init__(config)
        
        self.n_modes = n_modes
        
        if transition_matrix is None:
            self.tpm = 0.95 * np.eye(n_modes) + 0.05 / n_modes
        else:
            self.tpm = transition_matrix
        
        # Process noise for each mode (CV, CT-light, CT-heavy)
        self.mode_process_noise = [0.1, 1.0, 5.0]
    
    def _predict_all(self, dt: float):
        """Predict with IMM for each track."""
        for hyp in self.hypotheses:
            for track in hyp.tracks.values():
                # For simplicity, use single mode prediction
                # Full implementation would maintain mode probabilities per track
                super()._predict_all.__wrapped__(self, dt)


# =============================================================================
# MAIN - DEMONSTRATION
# =============================================================================

if __name__ == "__main__":
    print("="*80)
    print("NX-MIMOSA MHT (MULTIPLE HYPOTHESIS TRACKING)")
    print("="*80)
    
    # Configuration
    config = MHTConfig(
        gate_probability=0.9999,
        pd=0.9,
        lambda_fa=1e-6,
        lambda_new=1e-7,
        n_scan_pruning=3,
        max_hypotheses=50,
        m_of_n_init=(2, 3),
        n_miss_delete=5,
    )
    
    print(f"\nðŸ“‹ Configuration:")
    print(f"  Detection probability: {config.pd}")
    print(f"  N-scan pruning depth: {config.n_scan_pruning}")
    print(f"  Max hypotheses: {config.max_hypotheses}")
    print(f"  Track init: {config.m_of_n_init[0]}-of-{config.m_of_n_init[1]}")
    print(f"  Track delete: {config.n_miss_delete} consecutive misses")
    
    # Create MHT filter
    mht = MHTFilter(config)
    
    # Simulate multiple scans
    np.random.seed(42)
    
    # True target trajectories
    true_targets = [
        # Target 1: Moving east
        {'x0': np.array([0, 0, 10000]), 'v': np.array([200, 0, 0])},
        # Target 2: Moving north
        {'x0': np.array([5000, 0, 8000]), 'v': np.array([0, 150, 0])},
        # Target 3: Crossing (appears at scan 3)
        {'x0': np.array([2000, 2000, 9000]), 'v': np.array([100, -100, 0]), 'start_scan': 3},
    ]
    
    R = np.diag([50**2, 50**2, 100**2])  # Measurement noise
    n_scans = 10
    dt = 1.0
    
    print(f"\nðŸŽ¯ Simulating {len(true_targets)} targets over {n_scans} scans...")
    
    for scan in range(n_scans):
        print(f"\n--- Scan {scan + 1} ---")
        
        # Generate measurements
        measurements = []
        meas_id = 0
        
        for i, target in enumerate(true_targets):
            if target.get('start_scan', 0) > scan:
                continue
            
            # True position
            t = (scan - target.get('start_scan', 0)) * dt
            true_pos = target['x0'] + target['v'] * t
            
            # Detection with probability pd
            if np.random.random() < config.pd:
                z = true_pos + np.random.multivariate_normal([0,0,0], R)
                measurements.append(Measurement(
                    measurement_id=meas_id,
                    z=z,
                    R=R,
                    timestamp=scan * dt
                ))
                meas_id += 1
        
        # Add false alarms
        n_fa = np.random.poisson(0.5)
        for _ in range(n_fa):
            fa_pos = np.array([
                np.random.uniform(-1000, 10000),
                np.random.uniform(-1000, 10000),
                np.random.uniform(5000, 15000)
            ])
            measurements.append(Measurement(
                measurement_id=meas_id,
                z=fa_pos,
                R=R,
                timestamp=scan * dt
            ))
            meas_id += 1
        
        print(f"  Measurements: {len(measurements)} (including {n_fa} false alarms)")
        
        # Process with MHT
        confirmed_tracks = mht.process(measurements, dt)
        
        print(f"  Hypotheses: {len(mht.hypotheses)}")
        print(f"  Confirmed tracks: {len(confirmed_tracks)}")
        
        for track in confirmed_tracks:
            print(f"    Track {track.track_id}: pos=({track.state[0]:.0f}, {track.state[1]:.0f}, {track.state[2]:.0f})")
    
    # Final results
    print(f"\n" + "="*80)
    print("FINAL RESULTS")
    print("="*80)
    
    print(f"\nðŸ“Š Statistics:")
    print(f"  Total scans processed: {mht.scan_count}")
    print(f"  Final hypotheses: {len(mht.hypotheses)}")
    
    best_tracks = mht.get_best_tracks()
    print(f"\nðŸŽ¯ Best Hypothesis Tracks ({len(best_tracks)}):")
    
    for track in best_tracks:
        print(f"  Track {track.track_id}:")
        print(f"    Position: ({track.state[0]:.1f}, {track.state[1]:.1f}, {track.state[2]:.1f})")
        print(f"    Velocity: ({track.state[3]:.1f}, {track.state[4]:.1f}, {track.state[5]:.1f})")
        print(f"    Updates: {track.n_updates}, Misses: {track.n_misses}")
        print(f"    Status: {track.status.name}")
    
    track_probs = mht.get_track_probabilities()
    print(f"\nðŸ“ˆ Track Existence Probabilities:")
    for tid, prob in track_probs.items():
        print(f"  Track {tid}: {prob:.3f}")
    
    print("\n" + "="*80)
    print("âœ“ MHT implementation ready for multi-target tracking")
    print("="*80)
